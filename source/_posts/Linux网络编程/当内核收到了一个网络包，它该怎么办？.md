---
title: 当内核收到了一个网络包，它该怎么办？
date: '2025-01-11 13:45:50'
updated: '2025-01-17 18:38:42'
---
![](/images/c7263471a11f268f68f597335319bbdf.gif)

<font style="color:rgb(25, 27, 31);">在计算机网络的世界里，数据的传输与处理犹如一场精密的交响乐演奏。当内核收到了一个网络包，就如同指挥家收到了新的乐谱信号，接下来一系列的操作将决定这个‘音符’如何在系统中奏响。从硬件底层到软件上层，从网卡驱动到协议栈处理，每一个环节都紧密相扣，共同完成数据的接收、解析与传递。现在，让我们深入探究一下，当内核收到一个网络包后，究竟会发生什么。</font>

## <font style="color:rgb(25, 27, 31);">一、网卡 “迎接” 网络包</font>
### <font style="color:rgb(25, 27, 31);">1.1从网线到网卡</font>
<font style="color:rgb(25, 27, 31);">当网络包从网线抵达网卡时，网卡作为网络世界与计算机硬件的 “前沿哨所”，率先行动起来。它内置的直接内存访问（DMA）引擎就像一位高效的 “搬运工”，会迅速把网络包搬运到预先分配好的收包队列中，这个过程无需 CPU 过多操心，极大地减轻了 CPU 的负担。就好比快递到达快递站后，工作人员会先把包裹按照区域分类放置，等待后续处理。</font>

![](/images/d411948f56bc3b88cb9d77d74607d3eb.jpeg)

<font style="color:rgb(25, 27, 31);">这个数据包来了之后，现在只是一堆</font>**<font style="color:rgb(25, 27, 31);">电信号</font>**<font style="color:rgb(25, 27, 31);">，离内核程序的处理还十万八千里呢，需要先经历</font>**<font style="color:rgb(25, 27, 31);">网卡</font>**<font style="color:rgb(25, 27, 31);">这个硬件的折磨。我们把上图中的网卡放大看看。</font>

![](/images/fc498d25e72a950283ba86471500dd2a.jpeg)

<font style="color:rgb(25, 27, 31);">而且，这个过程完全不需要 CPU 参与，只需要</font>**<font style="color:rgb(25, 27, 31);">DMA</font>**<font style="color:rgb(25, 27, 31);">这个硬件设备，配合网卡这个硬件设备即可完成。当然，这个过程的前提是，</font>**<font style="color:rgb(25, 27, 31);">网卡驱动</font>**<font style="color:rgb(25, 27, 31);">需要在内存中申请一个缓冲区叫</font>**<font style="color:rgb(25, 27, 31);">sk_buffer</font>**<font style="color:rgb(25, 27, 31);">，然后把这个 sk_buffer 的地址告诉网卡，这样 DMA 才知道等网卡的缓冲区有数据到来时，把它拷贝到内存的什么位置上。具体过程展开如下：</font>

![](/images/1a175981676a6d7c3c5e02e5fc572c8a.jpeg)

### <font style="color:rgb(25, 27, 31);">1.2注册硬中断处理程序</font>
<font style="color:rgb(25, 27, 31);">紧接着，网卡会向 CPU 发送一个硬中断信号，如同大声呼喊：“有新包裹到啦，快来处理！” 这个硬中断信号具有极高的优先级，CPU 收到后，会立刻停下手头正在执行的其他任务，转而响应网卡的请求。不过，硬中断处理的事情相对简单且迅速，只为后续的处理做好铺垫，把真正耗时的 “细活” 交给软中断和其他模块去完成。</font>

<font style="color:rgb(25, 27, 31);">总之现在，这份数据包，已经从网卡内的缓冲区，然后通过 DMA 的方式，拷贝到了内存中的 sk_buffer 这个结构中。由于这个过程完全是由硬件完成的，所以下一步网卡该做的最后一件事，就是通知内核，让内核去处理这个数据。怎么通知呢？就是中断。网卡向 CPU 发起中断信号，CPU 打断当前的程序，根据中断号找到中断处理程序，开始执行。那我们主要去看，这个网卡收包这个中断处理程序是什么，以及它是如何注册到中断向量表中的。由于各个类型的网卡驱动程序是不同的，这里我们拿 e1000 这个网卡驱动来举例。我们在 e1000_main.c 中找到了这样一行代码。</font>

```c
request_irq(netdev->irq, &e1000_intr, ...);
```

<font style="color:rgb(25, 27, 31);">这段代码的作用就是，当数据包从网卡缓冲区到内存中的 sk_buffer 后发出中断，将会执行到</font>**<font style="color:rgb(25, 27, 31);">e1000_intr</font>**<font style="color:rgb(25, 27, 31);">这个中断处理函数。</font>

## <font style="color:rgb(25, 27, 31);">二、中断处理程序 “接手”</font>
<font style="color:rgb(25, 27, 31);">CPU 响应硬中断后，就轮到中断处理程序登场了。它可是个 “急性子”，首先要做的就是为刚抵达的这个网络帧分配一个叫做 sk_buff（套接字缓冲区，socket buffer 的简称）的结构体，这个结构体就像是一个 “多功能收纳盒”，它不仅能存放网络包的数据，还附带了许多与网络包相关的重要信息，诸如源地址、目的地址、协议类型等等。接着，中断处理程序会迅速把网卡收包队列中的数据拷贝到 sk_buff 缓冲区中，有了这个 “收纳盒”，后续处理数据就方便多了。</font>

<font style="color:rgb(25, 27, 31);">不过，中断处理程序也明白 “好钢用在刀刃上” 的道理，它不会在这时候耗费过多时间去深度处理数据，毕竟 CPU 还有其他紧急任务等着处理呢。所以，在完成 sk_buff 的分配与数据拷贝后，中断处理程序就会发起一个软中断，然后潇洒地把接力棒交给软中断去处理，自己则快速退出，好让 CPU 能尽快恢复其他工作。这就好比快递站收到包裹后，简单登记入库，就通知快递员去派送一样，专业分工，高效协作。</font>

**<font style="color:rgb(25, 27, 31);">硬中断 e1000_intr 干了什么？</font>**

<font style="color:rgb(25, 27, 31);">drivers\net\e1000\e1000_main.c</font>

```c
// 注册的硬中断处理函数
static irqreturn_t e1000_intr(int irq, void *data, struct pt_regs *regs) {
   __netif_rx_schedule(netdev);
}
include\linux\netdevice.h
static inline void __netif_rx_schedule(struct net_device *dev) {
    list_add_tail(&dev->poll_list, &__get_cpu_var(softnet_data).poll_list);
    // 发出软中断    
    __raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
```

<font style="color:rgb(25, 27, 31);">没错，几乎啥也没干，将网卡设备 dev 放入</font>**<font style="color:rgb(25, 27, 31);">poll_list</font>**<font style="color:rgb(25, 27, 31);">里，然后立刻发起了一次</font>**<font style="color:rgb(25, 27, 31);">软中断</font>**<font style="color:rgb(25, 27, 31);">，然后就结束了。软中断原理这里不叙述，其实就是修改</font>**<font style="color:rgb(25, 27, 31);">pending</font>**<font style="color:rgb(25, 27, 31);">的某个标志位，然后内核中有一个线程不断</font>**<font style="color:rgb(25, 27, 31);">轮询</font>**<font style="color:rgb(25, 27, 31);">这组标志位，看哪个是 1 了，就去</font>**<font style="color:rgb(25, 27, 31);">软中断向量表</font>**<font style="color:rgb(25, 27, 31);">里，寻找这个标志位对应的处理程序，然后执行它。</font>

## <font style="color:rgb(25, 27, 31);">三、软中断 “唤醒” 内核协议栈</font>
<font style="color:rgb(25, 27, 31);">这是为了尽快响应硬中断，以便计算机可以尽快处理下一个硬中断，毕竟鼠标点击、键盘敲击等需要响应特别及时。而像网络包到来后的拷贝和解析过程，在硬中断面前优先级没那么高，所以就触发一个软中断等着内核线程去执行就好了。收到软中断信号后，就轮到内核协议栈 “大展身手” 了。</font>

<font style="color:rgb(25, 27, 31);">内核协议栈如同一个经验丰富的 “安检员”，它会从 sk_buff 缓冲区中取出网络帧，然后按照链路层、网络层、传输层的顺序，逐层对网络包进行仔细的检查与处理。</font>

<font style="color:rgb(25, 27, 31);">在链路层，它会严谨地检查报文的合法性，像是查看帧的校验和是否正确，确保数据没有在传输过程中出现损坏。同时，它还会精准地找出上层协议的类型，判断这个包是基于 IPv4 还是 IPv6 协议的，接着去掉帧头、帧尾这些传输过程中附加的 “包装”，把 “纯净” 的数据交给网络层。</font>

<font style="color:rgb(25, 27, 31);">网络层拿到数据后，会取出 IP 头，依据其中的信息判断网络包下一步该何去何从，是交给上层的应用程序处理，还是需要转发到其他设备。当确认这个包是要发送到本机后，网络层就会取出上层协议的类型，比如是 TCP 还是 UDP，然后去掉 IP 头，把接力棒递给传输层。</font>

<font style="color:rgb(25, 27, 31);">传输层接过网络包后，会取出 TCP 头或者 UDP 头，根据 <源 IP、源端口、目的 IP、目的端口> 四元组作为独一无二的标识，迅速找出对应的 Socket。就好比快递员根据收件地址找到对应的收件人家一样，精准无误。找到后，把数据拷贝到 Socket 的接收缓存中，至此，网络包已经顺利抵达了 “家门口”，就等着应用程序来取用了。</font>

### <font style="color:rgb(25, 27, 31);">3.1注册软中断处理程序</font>
<font style="color:rgb(25, 27, 31);">刚刚代码中我们就触发了一个值为</font>**<font style="color:rgb(25, 27, 31);">NET_RX_SOFTIRQ</font>**<font style="color:rgb(25, 27, 31);">的软中断，那这个软中断会执行到哪个软中断处理函数呢？内核早在网络子系统初始化的过程中，把这个软中断对应的处理函数注册好了。net\core\dev.c</font>

```c
static int __init net_dev_init(void) {
    open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL);
    open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL);
}

// transmit 发送
static void net_tx_action(struct softirq_action *h) {...}
// receive 接收
static void net_rx_action(struct softirq_action *h) {...}
```

<font style="color:rgb(25, 27, 31);">这个</font>**<font style="color:rgb(25, 27, 31);">open_softirq</font>**<font style="color:rgb(25, 27, 31);">就是注册一个软中断函数，很简单，就是把这个函数赋值给</font>**<font style="color:rgb(25, 27, 31);">软中断向量表</font>**<font style="color:rgb(25, 27, 31);">中对应位置的 action 上。</font>

![](/images/44065038c9ac33eb2ad3c4fbbab51f21.jpeg)

<font style="color:rgb(25, 27, 31);">这里注册了两个软中断，一个发送，一个接收。我们这次是接收，所以软中断触发后，就执行到了</font>**<font style="color:rgb(25, 27, 31);">net_rx_action</font>**<font style="color:rgb(25, 27, 31);">这个函数。</font>

### <font style="color:rgb(25, 27, 31);">3.2软中断 net_rx_action 干了什么</font>
<font style="color:rgb(25, 27, 31);">直接看！net\core\dev.c</font>

```c
static void net_rx_action(struct softirq_action *h) {
    struct softnet_data *queue = &__get_cpu_var(softnet_data);   
    while (!list_empty(&queue->poll_list)) {
        struct net_device dev = list_entry(
            queue->poll_list.next, struct net_device, poll_list);
        dev->poll(dev, &budget)；
        }
}
```

<font style="color:rgb(25, 27, 31);">遍历 poll_list 取出一个个的设备 dev，然后调用其</font>**<font style="color:rgb(25, 27, 31);">poll</font>**<font style="color:rgb(25, 27, 31);">函数。</font>

<font style="color:rgb(25, 27, 31);">还记得我们发起软中断前的一行代码吧？正是把当前有数据包到来的这个网卡设备 dev 放入了这个 poll_list，现在又取出来了。由于要调用该网卡相应驱动的 poll 函数，那网卡初始化时，e1000 这款网卡的 poll 函数被附上了这个函数地址。</font>

```c
netdev->poll = &e1000_clean;
```

<font style="color:rgb(25, 27, 31);">所以，接下来就看这个函数就好了，听名字就知道是清理这个网卡的数据包的工作。drivers\net\e1000\e1000_main.c</font>

```c
static int e1000_clean(struct net_device *netdev, int *budget) {
    struct e1000_adapter *adapter = netdev->priv;    
    e1000_clean_tx_irq(adapter);
    e1000_clean_rx_irq(adapter, &work_done, work_to_do);
}
```

<font style="color:rgb(25, 27, 31);">由于本讲我们只看读数据的过程，所以就看 rx 部分就好了。这个函数过长，我们只顺着一条线往下跟。</font>

```c
// drivers\net\e1000\e1000_main.c
e1000_clean_rx_irq(struct e1000_adapter *adapter) {
    ...
        netif_receive_skb(skb);
    ...
    }

// net\core\dev.c
int netif_receive_skb(struct sk_buff *skb) {
    ...
        list_for_each_entry_rcu(ptype, &ptype_base[ntohs(type)&15], list) {
        ...
            deliver_skb(skb, ptype, 0);
        ...
        }
    ...
    }

static __inline__ int deliver_skb(
    struct sk_buff *skb, struct packet_type *pt_prev, int last) {
    ...
        return pt_prev->func(skb, skb->dev, pt_prev);
}
```

<font style="color:rgb(25, 27, 31);">我们看到，一路跟来，执行了 pt_prev 的func函数。这个函数是干嘛的呢？或者先问，这个函数具体的实现指向的是哪个函数呢？这就涉及到协议栈的注册。</font>

### <font style="color:rgb(25, 27, 31);">3.3协议栈的注册</font>
```c
// net\ipv4\ip_output.c
static struct packet_type ip_packet_type = {
    .type = __constant_htons(ETH_P_IP),
    .func = ip_rcv,
};

void __init ip_init(void) {
    dev_add_pack(&ip_packet_type);
}

// net\core\dev.c
void dev_add_pack(struct packet_type *pt) {
    if (pt->type == htons(ETH_P_ALL)) {
        list_add_rcu(&pt->list, &ptype_all);
    } else {
        hash = ntohs(pt->type) & 15;
        list_add_rcu(&pt->list, &ptype_base[hash]);
    }
}
```

<font style="color:rgb(25, 27, 31);">我们看到，func 被赋值为了ip_rcv，那上一步自然就执行到了这个函数，其实就是网络层交给谁来负责解析的意思。那我们顺便把传输层的协议注册也看了吧，不难想到，ip_rcv 这个函数处理完必然交给传输层继续处理。</font>

```c
module_init(inet_init);

static struct inet_protocol tcp_protocol = {
    .handler =  tcp_v4_rcv,
    .err_handler =  tcp_v4_err,
    .no_policy =    1,
};

static struct inet_protocol udp_protocol = {
    .handler =  udp_rcv,
    .err_handler =  udp_err,
    .no_policy =    1,
};

static int __init inet_init(void) {
    inet_add_protocol(&udp_protocol, IPPROTO_UDP);
    inet_add_protocol(&tcp_protocol, IPPROTO_TCP);
    ip_init();
    tcp_init();
}
```

<font style="color:rgb(25, 27, 31);">非常直观明了，记住上面两个 handler 分别是</font>**<font style="color:rgb(25, 27, 31);">tcp_v4_rcv</font>**<font style="color:rgb(25, 27, 31);">和</font>**<font style="color:rgb(25, 27, 31);">udp_rcv</font>**<font style="color:rgb(25, 27, 31);">。</font>

## <font style="color:rgb(25, 27, 31);">四、数据 “奔赴” 应用程序</font>
<font style="color:rgb(25, 27, 31);">经过内核协议栈的层层 “安检”，网络包终于来到了应用程序的 “家门口”。此时，应用程序正通过 Socket 接口焦急地等待着数据的到来，就像等待快递上门的收件人。内核会依据 < 源 IP、源端口、目的 IP、目的端口 > 四元组，精准地找到对应的 Socket，然后把数据拷贝到 Socket 的接收缓存中。</font>

<font style="color:rgb(25, 27, 31);">至此，网络包已经成功 “入住” 接收缓存，就等应用程序前来读取，开启后续的数据处理流程，为用户提供丰富多彩的网络服务，比如展示网页内容、播放视频音频等等。整个过程，内核就像是一位严谨高效的 “管家”，有条不紊地打理着网络包的接收事务，确保每一个数据包都能准确无误地找到归宿，让网络世界顺畅运行。</font>

![](/images/a57d2c97cc1b1cf071876ec8901769c5.jpeg)

<font style="color:rgb(25, 27, 31);">随着网络技术的飞速发展，人们对网络性能的要求越来越高，这就促使内核网络包处理流程需要不断优化。一方面，优化方向聚焦在减少中断开销、优化协议栈处理、提升内存管理效率等方面。例如，采用 NAPI（New API）机制，它结合中断驱动和轮询机制，在高负载时，网卡只需产生一次中断，然后驱动程序进入轮询模式，批量处理数据包，减少了中断次数，降低了 CPU 的中断处理开销，大大提高了系统的网络吞吐量。</font>

<font style="color:rgb(25, 27, 31);">另一方面，内核网络包处理也面临诸多挑战。在高并发场景下，大量的网络包瞬间涌入，可能导致收包队列溢出、内核协议栈处理不及，进而引发丢包现象。而且，随着网络应用的多元化，不同协议、不同类型的数据包混杂，如何确保内核能快速、准确地分类处理，也是亟待解决的难题。</font>

<font style="color:rgb(25, 27, 31);">此外，硬件性能的瓶颈，如网卡速率、CPU 处理能力，也会对网络包处理流程造成限制。不过，正是这些挑战促使着技术人员不断探索创新，推动内核网络处理技术持续向前发展，让我们的网络世界更加顺畅、高效。</font>  


> 来自: [当内核收到了一个网络包，它该怎么办？](https://mp.weixin.qq.com/s/hRgpl9LwIayyhU-wIob6hQ)
>

